//
//  AIVoiceExampleView.swift
//  AIKit (Generated by SwiftyLaunch 1.5.0)
//  https://docs.swiftylaun.ch/module/aikit/ai-translator-example
//

import AnalyticsKit
import FirebaseKit
import SharedKit
import SwiftUI

struct AIVoiceExampleView: View {

	/// Environment Object to access the DB and Backend Functions
	@EnvironmentObject var db: DB

	/// View Model to control this View
	@StateObject private var vm = AIVoiceExampleViewModel()

	/// View Model to control the voice recording
	@StateObject private var voiceRecordingVM = VoiceRecordingViewModel()

	var body: some View {

		VStack {

			Spacer()

			Button {
				// Translate button released -> call the function to translate the voice recording
				vm.releasedTranslateButton(voiceRecordingVM: voiceRecordingVM)
			} label: {
				Circle()
					.fill(Color.accentColor.gradient)
					.frame(width: 150, height: 150)
					.overlay {
						Image(systemName: "mic.fill")
							.foregroundStyle(.white)
							.font(.system(size: 75, weight: .semibold))
							.scaleEffect(voiceRecordingVM.isCurrentlyRecording ? 1 : 0.65)
					}
			}

			.buttonStyle(TransateButton())

			// Disable the button if we are currently processing
			.disabled(vm.processing)
			.opacity((vm.processing) ? 0.5 : 1)

			.overlay {
				// If we are processing -> show a loading indicator
				if vm.processing {
					ProgressView()
				}
			}

			// If the user presses for at least 0.2 and gave us microphone access -> call our voiceRecording function that will initiate recording
			.simultaneousGesture(
				LongPressGesture(minimumDuration: 0.2).onEnded { _ in
					askUserFor(.microphoneAccess) {
						voiceRecordingVM.shouldStartRecording()
					} onDismiss: {
						// Otherwise, if cancelled, show a notification that we need microphone access for this feature
						showInAppNotification(
							.warning,
							content: .init(
								title: "Microphone Access Required",
								message: "This Feature Requires Microphone Access"))
					}

				})

			Text(voiceRecordingVM.isCurrentlyRecording ? "Release to Translate" : "Hold to Speak")
				.font(.largeTitle)
				.fontWeight(.bold)
				.foregroundStyle(Color.accentColor.gradient)

			Menu {
				Picker("Will translate to:", selection: $vm.selectedOutputLanguage) {
					ForEach(languages, id: \.self) {
						Text($0)
							.font(.title)
							.fontWeight(.bold)

					}
				}
			} label: {
				HStack {
					Text("Will translate to \(vm.selectedOutputLanguage)")
					Image(systemName: "chevron.up.chevron.down")
						.font(.caption)
				}
				.fontWeight(.semibold)
				.padding(.horizontal, 20)
				.padding(.vertical, 10)
				.background(Color(uiColor: .systemBackground))
				.clipShape(Capsule(style: .continuous))
			}
			.disabled(vm.processing)

			Spacer()
		}
		.frame(maxWidth: .infinity, maxHeight: .infinity)
		.accentBackground(strong: true)

		.navigationTitle("Voice Example")

		// If we detect a currentAudioTranscription change -> tell that to the viewModel
		.onChange(of: voiceRecordingVM.currentAudioTranscription) {
			vm.detectedAudioTranscriptionUpdate(db: db, voiceRecordingVM: voiceRecordingVM)
		}

		.captureViewActivity(as: "AIVoiceExampleView")
	}
}

public struct TransateButton: ButtonStyle {

	public init() {}

	public func makeBody(configuration: Configuration) -> some View {
		configuration.label
			.overlay {
				Circle()
					.strokeBorder(Color.white, lineWidth: configuration.isPressed ? 7.5 : 5)
					.padding(5)
					.animation(.interactiveSpring(duration: 0.25), value: configuration.isPressed)
			}
			.hoverEffect()
	}
}

#Preview {
	NavigationStack {
		AIVoiceExampleView()
			.environmentObject(DB())
	}
}
