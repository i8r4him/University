//
//  AIVisionExampleViewModel.swift
//  AIKit (Generated by SwiftyLaunch 1.5.0)
//  https://docs.swiftylaun.ch/module/aikit/ai-vision-example
//

import AnalyticsKit
import FirebaseKit
import PhotosUI
import SharedKit
import SwiftUI

// The View Name for Analytics Capture
private let viewName = "AIVisionExampleView"

// The ViewModel for the AIVisionExampleView
// The "glue" between code logic and the view.

class AIVisionExampleViewModel: ObservableObject {

	/// Is Currently Processing the Image? (Waiting for the server)
	@Published var processing: Bool = false
	{
		didSet {
			Analytics.capture(
				.info, id: "processing_state_changed_to_\(processing)", source: .aikit, fromView: viewName)
		}
	}

	/// The Permission Requirement
	@Published public private(set) var gotCameraAccess: Bool = false

	/// Contains the Image selected from the Library. Not used directly here, but only as a placeholder. See PhotosPicker below.
	@Published var selectedImagePickerItem: PhotosPickerItem? = nil

	@MainActor
	/// Called by .requireCapabilityPermission View Modifier in AIVisionExampleView. If we got the camera permissions, we can start the camera session.
	func gotCameraPermissions(cameraVM: CameraViewModel) {
		withAnimation {
			gotCameraAccess = true
			cameraVM.configureCameraSession()
			cameraVM.setupBindings()
		}

	}

	/// User selected a photo from a library in AIVisionExampleView -> this gets called
	func photoSelectedFromLibrary(cameraVM: CameraViewModel) {

		/// Ignore if the newly selected image is nil
		if selectedImagePickerItem == nil { return }
		Task {

			///Load the selected tempor image from the library
			if let data = try? await selectedImagePickerItem?.loadTransferable(type: Data.self) {

				/// Start processing indication
				withAnimation { processing = true }

				/// Set the image to captured image, as if we just captured the library image with out camera
				await MainActor.run {
					cameraVM.capturedImage = UIImage(data: data)
				}

				/// Set the placeholder image back to nil
				selectedImagePickerItem = nil
			}
		}
	}

	/// Executed once a user presses on the shutter button and releases it
	func releasedShutterButton(cameraVM: CameraViewModel, voiceRecordingVM: VoiceRecordingViewModel) {
		Analytics.capture(.info, id: "released_shutter_button", source: .aikit, fromView: viewName)

		withAnimation {
			processing = true
		}

		/// If we are currently recording -> stop the recording -> which then generates a transcription, which we track to see if we should take a picture.
		/// Basically: user holds down button -> start recording -> user releases button -> stops recording -> transcription created -> we detect that change -> we capture a picture.
		/// If we are not recording, we just take a picture.
		if voiceRecordingVM.isCurrentlyRecording {

			///Because we only take a picture after we locally process the transcription, we want to tell the user to wait until the transcription is done
			showInAppNotification(
				.info, content: .init(title: "Hold Still", message: "Wait until you hear a shutter sound."))

			voiceRecordingVM.shouldStopRecording()
		} else {
			cameraVM.captureImage()
			Haptics.impact(style: .soft)
		}
	}

	@MainActor
	/// Is called in the AIVisionExampleView when it detects a cameraVM.capturedImage update in .onChange
	func detectedCapturedCameraImageUpdate(db: DB, cameraVM: CameraViewModel, voiceRecordingVM: VoiceRecordingViewModel) {

		guard let capturedImg = cameraVM.capturedImage else {
			withAnimation { processing = false }
			return
		}

		/// If for some reason we can't convert our captured image to base64, show an error and return
		guard let capturedImageBase64 = capturedImg.base64 else {
			showInAppNotification(.error, content: .init(title: "Vision Error", message: "Failed to capture image"))
			withAnimation { processing = false }
			return
		}

		Task {

			/// Send the image with the prompt to the server to process using OpenAI's API (see BackendFunctions & Backend Code)
			/// If the current audio description is nil (meaning we just snapped a picture and did not record users voice, we set the prompt to just describe the image, otherwise, we send user's voice transcription as the prompt)
			if let result = await db.processImageWithAI(
				jpegImageBase64: capturedImageBase64,
				processingCommand: voiceRecordingVM.currentAudioTranscription ?? "Describe exactly what you see.",
				readResultOutLoud: true)
			{

				/// If we got audio from the server (we should get it if we set `readResultOutLoud`to true), read the audio out loud. Otherwise show the result as a text in a notification
				if let audio = result.audio {
					voiceRecordingVM.generalAudioModel.playAudio(base64Source: audio)
				} else {
					showInAppNotification(
						content: .init(
							title: "Vision Result",
							message: LocalizedStringKey(stringLiteral: result.message)),
						style: .init(sfSymbol: "sparkles", symbolColor: .purple, size: .normal))
				}

			} else {
				/// Catch-all error saying that we failed to process the image
				showInAppNotification(
					.error, content: .init(title: "Vision Error", message: "Server Processing Error"))
			}

			/// After processing, failed or not, we want to set all the states back to their initial state
			voiceRecordingVM.currentAudioTranscription = nil
			cameraVM.capturedImage = nil

			/// And disable visible processing.
			withAnimation {
				processing = false
			}
		}
	}

	@MainActor
	/// Is called in the AIVisionExampleView when it detects a voiceRecordingVM.currentAudioTranscription update in .onChange
	/// Means that we have set transcribed all the speech -> snap a picture of the current view
	func detectedAudioTranscriptionUpdate(cameraVM: CameraViewModel, voiceRecordingVM: VoiceRecordingViewModel) {

		/// If it detects a change and its nil, means that we reset it and we just ignore it
		guard voiceRecordingVM.currentAudioTranscription != nil else {
			return
		}

		/// currentAudioTranscription set -> snap picture
		cameraVM.captureImage()
	}
}
